{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"M:/notebooks/Jonathan/Udacity_Project1/p1_navigation/Banana_Windows_x86_64/Banana_Windows_x86_64/Banana.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions for future work.\n",
    "#### The impletmentation of Prioritized Experience Replay, Noisy DQN, or distributional DQN...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************\n",
      "Loading Dueling Q Learning PyTorch Model\n",
      "****************************\n",
      "****************************\n",
      "Loading Dueling Q Learning Util\n",
      "****************************\n",
      "****************************\n",
      "Loading Dueling Q Learning Options\n",
      "****************************\n",
      "batch 64\n",
      "memory_size 1000000\n",
      "update_freq 64\n",
      "lr 0.0001\n",
      "discount_rate 0.9\n",
      "transfer_rate 0.001\n",
      "env Unity_Banana\n",
      "env_seed 0\n",
      "num_episodes 3000\n",
      "max_iteration 1000\n",
      "min_epsilon 0.1\n",
      "decay 0.995\n",
      "win_cond 13\n",
      "render True\n",
      "f C:\\Users\\jonathanoh\\AppData\\Roaming\\jupyter\\runtime\\kernel-e608cf32-e19b-438d-a402-4cb164434a7c.json\n",
      "tutu\n"
     ]
    }
   ],
   "source": [
    "#Here is Double dueling with prioritized experience replay\n",
    "path=os.getcwd()\n",
    "path_Prioritized_DQN=path\n",
    "sys.path.append(path_Prioritized_DQN)\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from agent import Agent\n",
    "import util\n",
    "\n",
    "from options import options\n",
    "options = options()\n",
    "\n",
    "\n",
    "opts = options.parse()\n",
    "for arg in vars(opts):\n",
    "    print( arg, getattr(opts, arg))\n",
    "agent = Agent(state_size,action_size , opts=opts, seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write 0\n",
      "write 1\n",
      "write 2\n",
      "write 3\n",
      "write 4\n",
      "write 5\n",
      "write 6\n",
      "write 7\n",
      "write 8\n",
      "write 9\n",
      "write 10\n",
      "write 11\n",
      "write 12\n",
      "write 13\n",
      "write 14\n",
      "write 15\n",
      "write 16\n",
      "write 17\n",
      "write 18\n",
      "write 19\n",
      "write 20\n",
      "write 21\n",
      "write 22\n",
      "write 23\n",
      "write 24\n",
      "write 25\n",
      "write 26\n",
      "write 27\n",
      "write 28\n",
      "write 29\n",
      "write 30\n",
      "write 31\n",
      "write 32\n",
      "write 33\n",
      "write 34\n",
      "write 35\n",
      "write 36\n",
      "write 37\n",
      "write 38\n",
      "write 39\n",
      "write 40\n",
      "write 41\n",
      "write 42\n",
      "write 43\n",
      "write 44\n",
      "write 45\n",
      "write 46\n",
      "write 47\n",
      "write 48\n",
      "write 49\n",
      "write 50\n",
      "write 51\n",
      "write 52\n",
      "write 53\n",
      "write 54\n",
      "write 55\n",
      "write 56\n",
      "write 57\n",
      "write 58\n",
      "write 59\n",
      "write 60\n",
      "write 61\n",
      "write 62\n",
      "write 63\n",
      "[999999, 1000000, 1000001, 1000002, 1000003, 1000004, 1000005, 1000006, 1000007, 1000008, 1000009, 1000010, 1000011, 1000012, 1000013, 1000014, 1000015, 1000016, 1000017, 1000018, 1000019, 1000020, 1000021, 1000022, 1000023, 1000024, 1000025, 1000026, 1000027, 1000028, 1000029, 1000030, 1000031, 1000032, 1000033, 1000034, 1000035, 1000036, 1000037, 1000038, 1000039, 1000040, 1000041, 1000042, 1000043, 1000044, 1000045, 1000046, 1000047, 1000048, 1000049, 1000050, 1000051, 1000052, 1000053, 1000054, 1000055, 1000056, 1000057, 1000058, 1000059, 1000060, 1000061, 1000062]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:\n",
      "UnicodeDecodeError while processing traceback.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (64,1) could not be broadcast to indexing result of shape (64,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "def Prioritized_DQN(num_episodes = opts.num_episodes, max_iteration = opts.max_iteration, init_epsilon = 1.0, min_epsilon = opts.min_epsilon, decay = opts.decay):\n",
    "    '''\n",
    "    :param num_episodes:\n",
    "    :param max_iteration:\n",
    "    :param init_epsilon:\n",
    "    :param min_epsilon:\n",
    "    :param decay:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    total_reward = []\n",
    "    total_reward_window = deque(maxlen=100)\n",
    "    epsilon = init_epsilon\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        rewards = 0\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]            # get the current state\n",
    "        toto=0\n",
    "        for k in range(max_iteration):\n",
    "\n",
    "            action = (agent.act(state, epsilon)).astype(int)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0] \n",
    "            done = env_info.local_done[0]\n",
    "            #print(toto)\n",
    "            #toto+=1\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            rewards += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        total_reward_window.append(rewards)\n",
    "        total_reward.append(rewards)\n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i, np.mean(total_reward_window)), end=\"\")\n",
    "        if i % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i, np.mean(total_reward_window)))\n",
    "\n",
    "        if np.mean(total_reward_window) >= opts.win_cond:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i ,\n",
    "                                                                                         np.mean(total_reward_window)))\n",
    "            torch.save(agent.local_model.state_dict(), path_Prioritized_DQN+'\\\\'+'checkpoint.pth')\n",
    "            break\n",
    "\n",
    "    torch.save(agent.local_model.state_dict(), path_Prioritized_DQN+'\\\\'+'checkpoint_end.pth')\n",
    "    return total_reward\n",
    "\n",
    "scores_Prioritized_dqn = Prioritized_DQN()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores_Prioritized_dqn)), scores_Prioritized_dqn)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "PIK = \"scores_vanilla_dqn.dat\"\n",
    "with open(PIK, \"rb\") as f:\n",
    "    data= pickle.load(f)\n",
    "    scores_vanilla_dqn=pd.Series(np.array(data))\n",
    "PIK = \"scores_double_dqn.dat\"\n",
    "with open(PIK, \"rb\") as f:\n",
    "    data= pickle.load(f)\n",
    "    scores_double_dqn=pd.Series(np.array(data))\n",
    "PIK = \"scores_dueling_dqn.dat\"\n",
    "with open(PIK, \"rb\") as f:\n",
    "    data= pickle.load(f)\n",
    "    scores_dueling_dqn=pd.Series(np.array(data))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot( scores_vanilla_dqn.rolling(100).mean(), label='Vanilla')\n",
    "plt.plot( scores_double_dqn.rolling(100).mean(), label='Double')\n",
    "plt.plot( scores_dueling_dqn.rolling(100).mean(), label='Dueling')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
